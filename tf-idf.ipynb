{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f056de",
   "metadata": {},
   "source": [
    "물론이죠! TF-IDF(Term Frequency-Inverse Document Frequency)는 문서 내 단어의 중요도를 정량화하는 대표적인 방법입니다. 아래에 개념, 수식, 그리고 간단한 예제를 같이 설명할게요.\n",
    "\n",
    "✅ 1. 개념 정리\n",
    "TF (Term Frequency)\n",
    "단어가 문서에서 얼마나 자주 나오는지 나타냅니다.\n",
    "\n",
    "𝑇\n",
    "𝐹\n",
    "(\n",
    "𝑡\n",
    ",\n",
    "𝑑\n",
    ")\n",
    "=\n",
    "단어 \n",
    "𝑡\n",
    " 의 빈도\n",
    "문서 \n",
    "𝑑\n",
    " 의 총 단어 수\n",
    "TF(t,d)= \n",
    "문서 d 의 총 단어 수\n",
    "단어 t 의 빈도\n",
    "​\n",
    " \n",
    "IDF (Inverse Document Frequency)\n",
    "단어가 전체 문서 집합에서 얼마나 드문지를 나타냅니다.\n",
    "\n",
    "𝐼\n",
    "𝐷\n",
    "𝐹\n",
    "(\n",
    "𝑡\n",
    ")\n",
    "=\n",
    "log\n",
    "⁡\n",
    "(\n",
    "𝑁\n",
    "1\n",
    "+\n",
    "𝑑\n",
    "𝑓\n",
    "(\n",
    "𝑡\n",
    ")\n",
    ")\n",
    "IDF(t)=log( \n",
    "1+df(t)\n",
    "N\n",
    "​\n",
    " )\n",
    "𝑁\n",
    "N: 전체 문서 수\n",
    "\n",
    "𝑑\n",
    "𝑓\n",
    "(\n",
    "𝑡\n",
    ")\n",
    "df(t): 단어 t가 나타난 문서 수\n",
    "\n",
    "TF-IDF\n",
    "\n",
    "TF-IDF\n",
    "(\n",
    "𝑡\n",
    ",\n",
    "𝑑\n",
    ")\n",
    "=\n",
    "𝑇\n",
    "𝐹\n",
    "(\n",
    "𝑡\n",
    ",\n",
    "𝑑\n",
    ")\n",
    "×\n",
    "𝐼\n",
    "𝐷\n",
    "𝐹\n",
    "(\n",
    "𝑡\n",
    ")\n",
    "TF-IDF(t,d)=TF(t,d)×IDF(t)\n",
    "✅ 2. 간단한 예\n",
    "🔸 문서 3개:\n",
    "vbnet\n",
    "Copy\n",
    "Edit\n",
    "D1: \"I like machine learning\"\n",
    "D2: \"I like deep learning\"\n",
    "D3: \"I like pizza\"\n",
    "🔸 단어 후보: [\"I\", \"like\", \"machine\", \"deep\", \"learning\", \"pizza\"]\n",
    "🔸 Step 1: TF 계산 (문서 D1 기준)\n",
    "\"I\": 1 / 4 = 0.25\n",
    "\n",
    "\"like\": 1 / 4 = 0.25\n",
    "\n",
    "\"machine\": 1 / 4 = 0.25\n",
    "\n",
    "\"learning\": 1 / 4 = 0.25\n",
    "\n",
    "🔸 Step 2: IDF 계산 (전체 문서 기준)\n",
    "\"I\": 나타난 문서 수 = 3 → \n",
    "log\n",
    "⁡\n",
    "(\n",
    "3\n",
    "1\n",
    "+\n",
    "3\n",
    ")\n",
    "=\n",
    "log\n",
    "⁡\n",
    "(\n",
    "0.75\n",
    ")\n",
    "=\n",
    "log( \n",
    "1+3\n",
    "3\n",
    "​\n",
    " )=log(0.75)= 음수 (보통 stopword라 제거함)\n",
    "\n",
    "\"machine\": 1개 문서 → \n",
    "log\n",
    "⁡\n",
    "(\n",
    "3\n",
    "1\n",
    "+\n",
    "1\n",
    ")\n",
    "=\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1.5\n",
    ")\n",
    "≈\n",
    "0.405\n",
    "log( \n",
    "1+1\n",
    "3\n",
    "​\n",
    " )=log(1.5)≈0.405\n",
    "\n",
    "\"learning\": 2개 문서 → \n",
    "log\n",
    "⁡\n",
    "(\n",
    "3\n",
    "/\n",
    "3\n",
    ")\n",
    "=\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    ")\n",
    "=\n",
    "0\n",
    "log(3/3)=log(1)=0\n",
    "\n",
    "\"pizza\": 1개 문서 → \n",
    "log\n",
    "⁡\n",
    "(\n",
    "3\n",
    "/\n",
    "2\n",
    ")\n",
    "≈\n",
    "0.405\n",
    "log(3/2)≈0.405\n",
    "\n",
    "🔸 Step 3: TF-IDF 계산 (D1, 단어: \"machine\")\n",
    "TF = 0.25, IDF ≈ 0.405\n",
    "\n",
    "TF-IDF = 0.25 × 0.405 ≈ 0.101\n",
    "\n",
    "✅ 결론\n",
    "문서에서 자주 등장하고, 전체 문서에서 드물게 나오는 단어일수록 TF-IDF가 높습니다.\n",
    "\n",
    "TF는 문서 내 중요도, IDF는 전체 문서 내 희소성을 반영합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5677b06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deep' 'learning' 'like' 'machine' 'pizza']\n",
      "[[0.         0.54783215 0.42544054 0.72033345 0.        ]\n",
      " [0.72033345 0.54783215 0.42544054 0.         0.        ]\n",
      " [0.         0.         0.50854232 0.         0.861037  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\"I like machine learning\", \"I like deep learning\", \"I like pizza\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
